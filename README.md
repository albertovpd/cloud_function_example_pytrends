# Use Pytrends to schedule requests from a Google Cloud Function and storage it in Google Cloud Storage.

- This is part of a project, shown **as example** to use Google Cloud Function with Pub/Sub and Cloud Scheduler to request info from Pytrends and write it to Google Cloud Storage, through the gcsfs library.

Check the whole project here: 

**https://towardsdatascience.com/creation-of-an-etl-in-google-cloud-platform-for-automated-reporting-8a0309ee8a78**

- If you are using Cloud Function in this way, follow the architecture of this folder: everything in the same folder, all outputs pointing to "**../tmp**" (that folder will be generated by the Cloud Function).

- Zip the elements of this folder (**not the folder**, the scripts) with **requirements.txt**, **your json credentials** and upload it to GCF.

- While working on your pc, create a .env file in this folder and write there your environment variables:

        TOKEN_NAME="the name of your credentials.json"
        PROJECT_NAME="your project in Google Cloud"
        PROJECT_PATH="your bucket in GCS/the_csv_name_there.csv"
        PROJECT_TMP="../tmp/the_csv_you_will_create.csv"

When creating the cloud function, in **Advanced**, write your environment variables (TOKEN_NAME,TOKEN_PATH...And their values. All, names and values without declaring string type (witout " "))


        Something worth telling: Do not blindly embrace raw data from Google Trends. Check info about it, for example, here:
        - https://support.google.com/google-ads/thread/8389370?hl=en
        - https://github.com/GeneralMills/pytrends/issues/140


That means Pytrends is useless? Of course not. It looks like the following: 

- Google trends searches the maximum on the specified period, makes that maximum the 100% of Trend Index and everything else is averaged by that top. If you request information weekly, you will have a point with 100% of Trend Index each week.

- If you request a list of elements, all elements will be averaged by the top one.

- If you request each of your keywords separately, each keyword will be averaged on time by its own top.

        We encourage you to develop your own structures to avoid or reduce the Google marvelous noise on data.


The ETL involving this piece of script have been developed by Alex Masip and Alberto Vargas, members of the Data Team of Labelium España.

- https://www.linkedin.com/in/alexmasip/

- https://www.linkedin.com/in/alberto-vargas-pina/

For any doubts, contact us.
